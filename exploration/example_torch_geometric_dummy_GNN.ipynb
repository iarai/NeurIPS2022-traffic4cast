{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ea6e587",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "#  Copyright 2022 Institute of Advanced Research in Artificial Intelligence (IARAI) GmbH.\n",
    "#  IARAI licenses this file to You under the Apache License, Version 2.0\n",
    "#  (the \"License\"); you may not use this file except in compliance with\n",
    "#  the License. You may obtain a copy of the License at\n",
    "#  http://www.apache.org/licenses/LICENSE-2.0\n",
    "#  Unless required by applicable law or agreed to in writing, software\n",
    "#  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#  See the License for the specific language governing permissions and\n",
    "#  limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "027b2d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "874c135d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatevly, in order to make the module imports work properly set PYTHONPATH=$PWD before launching the notebook server from the repo root folder.\n",
    "sys.path.insert(0, os.path.abspath(\"../\"))  # noqa:E402"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e0a3f0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "![t4c20logo](../t4c20logo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89596492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "import tqdm\n",
    "from IPython.core.display import HTML\n",
    "from IPython.display import display\n",
    "from torch import nn\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "import t4c22\n",
    "from t4c22.metric.masked_crossentropy import get_weights_from_class_fractions\n",
    "from t4c22.misc.t4c22_logging import t4c_apply_basic_logging_config\n",
    "from t4c22.t4c22_config import class_fractions\n",
    "from t4c22.t4c22_config import load_basedir\n",
    "from t4c22.dataloading.t4c22_dataset_geometric import T4c22GeometricDataset\n",
    "from t4c22.plotting.plot_congestion_classification import plot_segment_classifications_simple\n",
    "from t4c22.misc.notebook_helpers import restartkernel  # noqa:F401\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "534fc667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time module is not an IPython extension.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(60000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 60 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%load_ext time\n",
    "%autoreload 2\n",
    "%autosave 60\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f14eba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "t4c_apply_basic_logging_config(loglevel=\"DEBUG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55f6be23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load BASEDIR from file, change to your data root\n",
    "BASEDIR = load_basedir(fn=\"t4c22_config.json\", pkg=t4c22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c41b940",
   "metadata": {},
   "outputs": [],
   "source": [
    "city = \"london\"\n",
    "# city = \"melbourne\"\n",
    "# city = \"madrid\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028f7b34",
   "metadata": {},
   "source": [
    "## Torch Geometric Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d49be49",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 927 ms, sys: 67.9 ms, total: 995 ms\n",
      "Wall time: 964 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataset = T4c22GeometricDataset(root=BASEDIR, city=city, split=\"train\", cachedir=Path(\"/tmp/processed\"))\n",
    "# train_dataset = T4c22GeometricDataset(root=BASEDIR, city=city, split=\"train\", cachedir=Path(\"/tmp/processed5\"), limit=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bab7db5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7040"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0cfe922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.51 ms, sys: 0 ns, total: 1.51 ms\n",
      "Wall time: 1.27 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Data(x=[59110, 4], edge_index=[2, 132414], y=[132414])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# 2.41s -> 2.35ms from cachedir!!\n",
    "dataset.get(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b05e8a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5632"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spl = int(((0.8 * len(dataset)) // 2) * 2)\n",
    "spl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "154ab0b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataset.Subset at 0x7f0dae03cb50>,\n",
       " <torch.utils.data.dataset.Subset at 0x7f0e053d2730>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, val_dataset = torch.utils.data.Subset(dataset,range(spl)), torch.utils.data.Subset(dataset,range(spl, len(dataset)))\n",
    "train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02a98191",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not (train_dataset[0].x.nan_to_num(-33) == val_dataset[0].x.nan_to_num(-33)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3f14bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2019-12-04', 24)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.dataset.day_t[train_dataset.indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0c44c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2019-08-03', 24)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset.dataset.day_t[val_dataset.indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d9f67db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7040, 7040)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset.dataset.day_t), len(val_dataset.dataset.day_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "163dac3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5632"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be7ee2c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1408"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d269b",
   "metadata": {},
   "source": [
    "## GNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b6ed1d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swish(nn.Module):\n",
    "    def __init__(self, beta=1):\n",
    "        super(Swish, self).__init__()\n",
    "        self.beta = beta\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(self.beta * x)\n",
    "\n",
    "\n",
    "class GNN_Layer(MessagePassing):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_features : int\n",
    "        Dimensionality of input features.\n",
    "    out_features : int\n",
    "        Dimensionality of output features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, hidden_features):\n",
    "        super(GNN_Layer, self).__init__(node_dim=-2, aggr=\"mean\")\n",
    "\n",
    "        self.message_net = nn.Sequential(\n",
    "            nn.Linear(2 * in_features, hidden_features), Swish(), nn.BatchNorm1d(hidden_features), nn.Linear(hidden_features, out_features), Swish()\n",
    "        )\n",
    "        self.update_net = nn.Sequential(nn.Linear(in_features + hidden_features, hidden_features), Swish(), nn.Linear(hidden_features, out_features), Swish())\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        \"\"\"Propagate messages along edges.\"\"\"\n",
    "        x = self.propagate(edge_index, x=x)\n",
    "        # x = self.norm(x, batch)\n",
    "        return x\n",
    "\n",
    "    def message(self, x_i, x_j):\n",
    "        \"\"\"Message update.\"\"\"\n",
    "        message = self.message_net(torch.cat((x_i, x_j), dim=-1))\n",
    "        return message\n",
    "\n",
    "    def update(self, message, x):\n",
    "        \"\"\"Node update.\"\"\"\n",
    "        x += self.update_net(torch.cat((x, message), dim=-1))\n",
    "        return x\n",
    "\n",
    "\n",
    "class CongestioNN(torch.nn.Module):\n",
    "    def __init__(self, in_features=4, out_features=32, hidden_features=32, hidden_layer=1):\n",
    "\n",
    "        super(CongestioNN, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.hidden_features = hidden_features\n",
    "        self.hidden_layer = hidden_layer\n",
    "\n",
    "        # in_features have to be of the same size as out_features for the time being\n",
    "        self.cgnn = torch.nn.ModuleList(modules=[GNN_Layer(self.out_features, self.out_features, self.hidden_features) for _ in range(self.hidden_layer)])\n",
    "\n",
    "        self.head_pre_pool = nn.Sequential(nn.Linear(self.out_features, self.hidden_features), Swish(), nn.Linear(self.hidden_features, self.hidden_features))\n",
    "        self.head_post_pool = nn.Sequential(nn.Linear(self.hidden_features, self.hidden_features), Swish(), nn.Linear(hidden_features, 1))\n",
    "\n",
    "        self.embedding_mlp = nn.Sequential(nn.Linear(self.in_features, self.out_features))\n",
    "\n",
    "    def forward(self, data):\n",
    "        batch = data.batch\n",
    "        x = data.x\n",
    "        edge_index = data.edge_index\n",
    "\n",
    "        x = self.embedding_mlp(x)\n",
    "        for i in range(self.hidden_layer):\n",
    "            x = self.cgnn[i](x, edge_index, batch)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class LinkPredictor(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, dropout):\n",
    "        super(LinkPredictor, self).__init__()\n",
    "\n",
    "        self.lins = torch.nn.ModuleList()\n",
    "        self.lins.append(torch.nn.Linear(in_channels, hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.lins.append(torch.nn.Linear(hidden_channels, hidden_channels))\n",
    "        self.lins.append(torch.nn.Linear(hidden_channels, out_channels))\n",
    "\n",
    "        self.swish = Swish()\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for lin in self.lins:\n",
    "            lin.reset_parameters()\n",
    "\n",
    "    def forward(self, x_i, x_j):\n",
    "        x = x_i * x_j\n",
    "        for lin in self.lins[:-1]:\n",
    "            x = lin(x)\n",
    "            x = self.swish(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.lins[-1](x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ec4493",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f2da7271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'green': 0.5367906303432076,\n",
       " 'yellow': 0.35138063340805714,\n",
       " 'red': 0.11182873624873524}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_class_fractions = class_fractions[city]\n",
    "city_class_fractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4315abf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6210, 0.9486, 2.9807])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_class_weights = torch.tensor(get_weights_from_class_fractions([city_class_fractions[c] for c in [\"green\", \"yellow\", \"red\"]])).float()\n",
    "city_class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e42033b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, predictor, dataset, optimizer, batch_size, device):\n",
    "    model.train()\n",
    "\n",
    "    losses = []\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for data in tqdm.notebook.tqdm(\n",
    "        torch_geometric.loader.dataloader.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=16),\n",
    "        \"train\",\n",
    "        total=len(dataset) // batch_size,\n",
    "    ):\n",
    "\n",
    "        data = data.to(device)\n",
    "\n",
    "        data.x = data.x.nan_to_num(-1)\n",
    "\n",
    "        h = model(data)\n",
    "        assert (h.isnan()).sum() == 0, h\n",
    "        x_i = torch.index_select(h, 0, data.edge_index[0])\n",
    "        x_j = torch.index_select(h, 0, data.edge_index[1])\n",
    "\n",
    "        y_hat = predictor(x_i, x_j)\n",
    "\n",
    "        y = data.y.nan_to_num(-1)\n",
    "        y = y.long()\n",
    "\n",
    "        loss_f = torch.nn.CrossEntropyLoss(weight=city_class_weights, ignore_index=-1)\n",
    "        loss = loss_f(y_hat, y)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        losses.append(loss.cpu().item())\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2b56cca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(model, predictor, validation_dataset, batch_size, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    y_hat_list = []\n",
    "    y_list = []\n",
    "\n",
    "    for data in tqdm.notebook.tqdm(validation_dataset, \"test\", total=len(validation_dataset)):\n",
    "        data = data.to(device)\n",
    "\n",
    "        data.x = data.x.nan_to_num(-1)\n",
    "        h = model(data)\n",
    "\n",
    "        x_i = torch.index_select(h, 0, data.edge_index[0])\n",
    "        x_j = torch.index_select(h, 0, data.edge_index[1])\n",
    "\n",
    "        y_hat = predictor(x_i, x_j)\n",
    "\n",
    "        y_hat_list.append(y_hat)\n",
    "        y_list.append(data.y)\n",
    "\n",
    "    y_hat = torch.cat(y_hat_list, 0)\n",
    "    y = torch.cat(y_list, 0)\n",
    "    y = y.nan_to_num(-1)\n",
    "    y = y.long()\n",
    "    loss = torch.nn.CrossEntropyLoss(weight=city_class_weights, ignore_index=-1)\n",
    "    total_loss = loss(y_hat, y)\n",
    "    print(f\"total losses {total_loss}\")\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "57c279ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_channels = 256\n",
    "num_layers = 10\n",
    "batch_size = 2\n",
    "eval_steps = 1\n",
    "epochs = 20\n",
    "runs = 1\n",
    "dropout = 0.0\n",
    "num_edge_classes = 3\n",
    "num_features = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bc869db7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a1e64ea27464aeabe8815dbf78ec5a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "runs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66fdd60ad9e949c29fd5b7197e5e5095",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epochs:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79c83844e56b431a879f732284439683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/2816 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [88]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mnotebook\u001b[38;5;241m.\u001b[39mtqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m epochs), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m, total\u001b[38;5;241m=\u001b[39mepochs):\n\u001b[0;32m---> 24\u001b[0m     losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     train_losses[(run, epoch)] \u001b[38;5;241m=\u001b[39m losses\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(statistics\u001b[38;5;241m.\u001b[39mmean(losses))\n",
      "Input \u001b[0;32mIn [41]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, predictor, dataset, optimizer, batch_size, device)\u001b[0m\n\u001b[1;32m     28\u001b[0m loss_f \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(weight\u001b[38;5;241m=\u001b[39mcity_class_weights, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     29\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_f(y_hat, y)\n\u001b[0;32m---> 31\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     34\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/.conda/envs/t4c22/lib/python3.10/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/t4c22/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = 0\n",
    "device = f\"cuda:{device}\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device)\n",
    "\n",
    "\n",
    "city_class_weights = city_class_weights.to(device)\n",
    "\n",
    "model = CongestioNN(num_features, hidden_channels, hidden_channels, num_layers)\n",
    "model = model.to(device)\n",
    "\n",
    "predictor = LinkPredictor(hidden_channels, hidden_channels, num_edge_classes, num_layers, dropout).to(device)\n",
    "\n",
    "train_losses = defaultdict(lambda: [])\n",
    "val_losses = defaultdict(lambda: -1)\n",
    "\n",
    "for run in tqdm.notebook.tqdm(range(runs), desc=\"runs\", total=runs):\n",
    "    # model.reset_parameters()\n",
    "    predictor.reset_parameters()\n",
    "    optimizer = torch.optim.AdamW(\n",
    "            [\n",
    "                {\"params\": model.parameters()},\n",
    "                {\"params\": predictor.parameters()}\n",
    "            ],\n",
    "            lr=5e-4,\n",
    "            weight_decay=0.001\n",
    "        )\n",
    "\n",
    "    for epoch in tqdm.notebook.tqdm(range(1, 1 + epochs), \"epochs\", total=epochs):\n",
    "        losses = train(model, predictor, dataset=train_dataset, optimizer=optimizer, batch_size=batch_size, device=device)\n",
    "        train_losses[(run, epoch)] = losses\n",
    "\n",
    "        print(statistics.mean(losses))\n",
    "        if epoch % eval_steps == 0:\n",
    "\n",
    "            val_loss = test(model, predictor, validation_dataset=val_dataset, batch_size=batch_size, device=device)\n",
    "            val_losses[(run, epoch)] = val_loss\n",
    "            print(f\"val_loss={val_loss} after epoch {epoch} of run {run}\")\n",
    "            torch.save(model.state_dict(), f\"GNN_model_{epoch:03d}.pt\")\n",
    "            torch.save(predictor.state_dict(), f\"GNN_predictor_{epoch:03d}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5326155b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for e, v in train_losses.items():\n",
    "    print(e)\n",
    "    print(statistics.mean(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdbcbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for e, v in val_losses.items():\n",
    "    print(e)\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5caaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# free resources by restarting kernel\n",
    "# restartkernel()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "text_representation": {
    "extension": ".py",
    "format_name": "light",
    "format_version": "1.5",
    "jupytext_version": "1.11.2"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
